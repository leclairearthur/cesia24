{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzo652OIfgkQ"
   },
   "source": [
    "# Wasserstein GAN for generation of MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS78gxFngGi-"
   },
   "source": [
    "This practical session is based on the [DCGAN Pytorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).\n",
    "\n",
    "It was adapted by\n",
    "* Lucía Bouza\n",
    "* Bruno Galerne\n",
    "* Arthur Leclaire\n",
    "\n",
    "You should complete the code regions marked with ###...###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds7HbRCmgCwu"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LHhzJBLSNOh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchvision.utils import make_grid\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "# !nvidia-smi\n",
    "\n",
    "# Displaying function\n",
    "def imshow(img,size=None):\n",
    "    img = img*0.5 + 0.5     # unnormalize\n",
    "    if size is not None:\n",
    "        img = transforms.Resize(size=size, interpolation=transforms.InterpolationMode.NEAREST, antialias=True)(img)\n",
    "    pil_img = transforms.functional.to_pil_image(img)\n",
    "    display(pil_img)\n",
    "    # print(\"Image size (h x w): \",  pil_img.height, \"x\", pil_img.width)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x11o4K94h6CY"
   },
   "source": [
    "## Download MNIST dataset\n",
    "\n",
    "Note that we normalize the images between -1 and 1 because during sampling, we have to limit the input space and scaling between -1 and 1 makes it easier to implement it. We discard the last batch so that all batches have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2Scj6lGiByO"
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Draw a batch of real images with the train_loader and display them. Use `next` and `iter` to get a batch from `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real,_ = next(iter(train_loader))\n",
    "print(real.shape)\n",
    "    \n",
    "pil_img = imshow(make_grid(real.to('cpu'),nrow=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5fs7sxuQyY8"
   },
   "source": [
    "## Generator and Discriminator Models\n",
    "\n",
    "The architecture of DCGAN is described in the [(Radford et al., 2016)](https://arxiv.org/pdf/1511.06434.pdf) \n",
    "\n",
    "QUESTION: Examine the architecture of the following generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbyuGWJQREbg"
   },
   "outputs": [],
   "source": [
    "# Size  of generator input\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator and discriminator\n",
    "ngf, ndf = 64, 64\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = nz, out_channels = ngf * 8, kernel_size = 4, stride = 1, padding = 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 8, out_channels = ngf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 4, out_channels = ngf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 2, out_channels = ngf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(in_channels = ngf, out_channels = 1, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "            # output size. 1 x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is 1 x 28 x 28\n",
    "            nn.Conv2d(in_channels = 1, out_channels = ndf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 15 x 15\n",
    "            nn.Conv2d(in_channels = ndf, out_channels= ndf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(in_channels = ndf * 2, out_channels = ndf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 5 x 5\n",
    "            nn.Conv2d(in_channels = ndf * 4, out_channels = 1, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibgOtzGg6Xx4"
   },
   "source": [
    "## Weight initialization\n",
    "\n",
    "The DCGAN [paper](https://arxiv.org/pdf/1511.06434.pdf) mentions that all model weights shall be randomly initialized from a Normal distribution with $\\mu=0$ and $\\sigma=0.02$. We implement `weights_init` function to reinitialize the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZDe_VPeRqTg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Create the generator and discriminator\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02.\n",
    "G.apply(weights_init);\n",
    "D.apply(weights_init);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Architectures of Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "\n",
    "# Create some generator and discriminator\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "torchsummary.summary(G, input_size=(nz,1,1))\n",
    "torchsummary.summary(D, input_size=(1,28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples of the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display samples of the generator\n",
    "def show(G,z=None,batch_size=128,nz=100):\n",
    "  # provide random latent code as option to see evolution\n",
    "  with torch.no_grad():\n",
    "    if z==None:\n",
    "      z = torch.randn(batch_size,nz,1,1).to(device)\n",
    "    genimages = G(z)\n",
    "    pil_img = imshow(utils.make_grid(genimages.to('cpu'),nrow=16))\n",
    "    return(pil_img)\n",
    "\n",
    "show(G);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: DCGAN Training with WGAN-GP loss\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Understand the following functions that compute the gradient penalty, and an estimation of the Lipschitz constant of D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(D,x,y):\n",
    "    # Calculate interpolation\n",
    "    b = x.shape[0]\n",
    "    if y.shape[0]!=b:\n",
    "        print('wrong size')\n",
    "    alpha = torch.rand((b,1,1,1),device=device)\n",
    "    interp = (alpha * y + (1 - alpha) * x)\n",
    "    interp.requires_grad_()\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    Di = D(interp).view(-1)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradout = torch.ones(Di.size()).to(device)\n",
    "    gradients = torch.autograd.grad(outputs=Di, inputs=interp, grad_outputs=gradout,\n",
    "       create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "    \n",
    "def lipconstant(D,x,y):\n",
    "    # Calculate interpolation between points in batches x and y (of same size b)\n",
    "    b = x.shape[0]\n",
    "    if y.shape[0]!=b:\n",
    "        print('wrong size')\n",
    "    alpha = torch.rand((b,1,1,1),device=device)\n",
    "    interp = alpha * y + (1 - alpha) * x\n",
    "    interp.requires_grad_()\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    Di = D(interp).view(-1)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradout = torch.ones(Di.size()).to(device)\n",
    "    gradients = torch.autograd.grad(outputs=Di, inputs=interp, grad_outputs=gradout,\n",
    "       create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1))\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return torch.mean(gradients_norm)\n",
    "\n",
    "\n",
    "y = next(iter(train_loader))[0].to(device)\n",
    "x = G(torch.randn(batch_size, nz, 1, 1, device=device)).detach()\n",
    "\n",
    "print(lipconstant(D,x,y))\n",
    "print(gradient_penalty(D,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GswgbnEDwviP"
   },
   "source": [
    "**QUESTION:** Implement WGAN-GP training for MNIST by completing the code in the following cell.\n",
    "We recall the pseudo-code:\n",
    "\n",
    "> For each batch of images $x_{\\text{real}}$:\n",
    ">\n",
    "> **1) Train discriminator:**\n",
    "> > Generate $z$ a tensor of size $b\\times nz\\times 1\\times 1$ of idd Gaussian variables  \n",
    "> > Generate  $x_{\\text{fake}} = \\mathtt{G}(z)$ a set $b$ fake images  \n",
    "> > Compute the discriminator loss to maximize <br/>\n",
    "> > Compute the gradient and do an optimizer step for the disciminator parameters  \n",
    ">\n",
    "> **2) Train the generator:**\n",
    "> > Generate $z$ a new tensor of size $b\\times nz\\times 1\\times 1$ of idd Gaussian variables  \n",
    "> > Compute the generator loss to minimize <br/>\n",
    "> > Compute the gradient and do an optimizer step for the generator parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
    "\n",
    "num_epochs = 5\n",
    "log_every = 100\n",
    "gpw = 0.1         # weight of gradient penalty term\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G.apply(weights_init);\n",
    "D.apply(weights_init);\n",
    "\n",
    "optimD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "zviz = torch.randn(batch_size,nz,1,1).to(device)\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the train_loader\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # Batchs of real and fake images\n",
    "        real = batch[0].to(device)\n",
    "        fake = G(torch.randn(batch_size, nz, 1, 1, device=device))\n",
    "        faked = fake.detach()\n",
    "\n",
    "        ############################\n",
    "        # Update D network\n",
    "        ### ... ###\n",
    "\n",
    "        ############################\n",
    "        # Update G network\n",
    "        ### ... ###\n",
    "\n",
    "        ############################\n",
    "        # Display training stats and visualize\n",
    "        if i % log_every == 0:\n",
    "            print('[%d/%d][%d/%d][%.4f s]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLip(D): %.4f' \n",
    "                  % (epoch+1, num_epochs, i, len(train_loader), time.time()-t0, Dloss.item(), Gloss.item(),lipconstant(D,real,faked)))\n",
    "            show(G,zviz)\n",
    "\n",
    "print('Total learning time = ',time.time()-t0)\n",
    "            \n",
    "# Save final generator in a variable for later use\n",
    "wgan = Generator()\n",
    "wgan.load_state_dict(G.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final generator for later use\n",
    "torch.save(G.state_dict(), 'wgan.pt')\n",
    "wgan = Generator()\n",
    "wgan.load_state_dict(G.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 BONUS: Let's play with the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXbp5DXM__0K"
   },
   "source": [
    "## Interpolation in latent space:\n",
    "\n",
    "**QUESTION:**\n",
    "Generate 2 sets of 10 latent variable $z_0$ and $z_1$ and display the generated images by the latent variables:\n",
    "$$\n",
    "z_\\alpha = (1-\\alpha) z_0 + \\alpha z_1\n",
    "$$\n",
    "for $\\alpha$ varying between $0$ and $1$.\n",
    "\n",
    "Display all the images in a grid of height 10 and width 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmGx53H7uvsa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You may load a trained generator from a file\n",
    "# G = Generator().to(device)\n",
    "# G.load_state_dict(torch.load('wgan_epoch100.pt'))\n",
    "# G.eval();  # Turn generator in evaluation mode to fix BatchNorm layers\n",
    "\n",
    "minib = 10\n",
    "nk = 30\n",
    "\n",
    "z0 = torch.randn(minib, nz, 1, 1, device=device)\n",
    "z1 = torch.randn(minib, nz, 1, 1, device=device)\n",
    "\n",
    "genimages = torch.zeros((minib*nk,1,28,28))\n",
    "for k in np.arange(nk):\n",
    "    ### ... ###\n",
    "\n",
    "pil_img = imshow(make_grid(genimages.to('cpu'),nrow=nk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the database\n",
    "train_loader_all = data.DataLoader(train_set, batch_size=60000, shuffle=False, num_workers=2, drop_last=True)\n",
    "y,labels = next(iter(train_loader_all))\n",
    "\n",
    "realzeros = y[labels==0]\n",
    "realones = y[labels==1]\n",
    "realtwos = y[labels==2]\n",
    "realthrees = y[labels==3]\n",
    "imshow(make_grid(realzeros[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(make_grid(realones[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(make_grid(realtwos[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(make_grid(realthrees[0:128,:,:,:].to('cpu'),nrow=16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** For several samples of the generative model, compute the nearest neighbors in the whole dataset.\n",
    "\n",
    "Display the samples and their nearest neighbor side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ... ###"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
